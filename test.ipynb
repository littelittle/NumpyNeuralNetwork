{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 12, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mynn import op as nn\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "# test 1 # \n",
    "# input = np.random.randn(batchsize, 10)\n",
    "# l1 = nn.Linear(10, 20)\n",
    "# l1(input).shape\n",
    "# grad = np.random.randn(batchsize, 20)\n",
    "# l1.backward(grad).shape\n",
    "\n",
    "# test 2 #\n",
    "def test2(verbose=False):\n",
    "    iter_time = 10000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(X)\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "        l2.backward(grad)\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 1*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"Need to break!!!\") \n",
    "        # print([np.linalg.norm(l2.params[key]) for key in l2.grads.keys()])\n",
    "    print(f\"the residual norm is {np.linalg.norm(l2.W-W_gt), np.linalg.norm(l2.b-b_gt)}\")\n",
    "\n",
    "def test3(verbose=False):\n",
    "    iter_time = 200000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l1 = nn.Linear(10, 10)\n",
    "    relu = nn.ReLU()\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(relu(l1(X)))\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "\n",
    "        # passing the grad to l2!\n",
    "        passing_grad = l2.backward(grad)\n",
    "        print(f\"norm of the grad is {np.linalg.norm(grad)}\")\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 0.01*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(f\"grad is {grad}\")\n",
    "                raise ValueError(\"l2 Need to break!!!\") \n",
    "            \n",
    "        # passing the grad to relu!\n",
    "        passing_grad = relu.backward(passing_grad)\n",
    "        # no params to optimize for relu!\n",
    "\n",
    "        # passing the grad to l1!\n",
    "        l1.backward(passing_grad)\n",
    "        print(np.linalg.norm(passing_grad))\n",
    "        for key in l1.grads.keys():\n",
    "            l1.params[key] -= 0.01*l1.grads[key]\n",
    "            if np.any(np.isnan(l1.params[key])):\n",
    "                print(l1.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"l1 Need to break!!!\")        \n",
    "\n",
    "def test4():\n",
    "    con1 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=5)\n",
    "    con2 = nn.Conv2D(in_channels=6, out_channels=12, kernel_size=5, stride=2)\n",
    "    X = np.random.rand(16, 3, 28, 28)\n",
    "    print(con2(con1(X)).shape)\n",
    "    # grad = np.zeros((16, 12, 13, 13))\n",
    "    # con2.backward(grad)\n",
    "\n",
    "def test4_backward(verbose=False):\n",
    "    iter_time = 1000\n",
    "    con1 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=4)\n",
    "    con2 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=4)\n",
    "    print(np.linalg.norm(con1.kernel-con2.kernel))\n",
    "    print(np.linalg.norm(con1.b-con2.b))\n",
    "    for i in range(iter_time):\n",
    "        X = np.random.rand(16, 3, 32, 32)\n",
    "        gt = con1(X)\n",
    "        pred = con2(X)\n",
    "        grad = -(gt-pred)\n",
    "        con2.backward(grad)\n",
    "        for key in con2.grads.keys():\n",
    "            con2.params[key] -= 0.1*con2.grads[key]\n",
    "        if verbose and i%100==0:\n",
    "            print(np.linalg.norm(con1.b-con2.b))\n",
    "    print(np.linalg.norm(con1.kernel-con2.kernel))\n",
    "    print(np.linalg.norm(con1.b-con2.b))\n",
    "\n",
    "\n",
    "\n",
    "def test5():\n",
    "    predict = np.random.rand(5, 10)\n",
    "    lable = np.array([2, 1, 4, 3, 6])\n",
    "    loss = nn.MultiCrossEntropyLoss()\n",
    "    print(loss(predicts=predict, labels=lable))\n",
    "    loss.backward()\n",
    "    print(loss.grads)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test4_backward(verbose=True)\n",
    "    # test2()\n",
    "    test4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CNN Model With Whose Sublayer is as below:\n",
      "                    ['A Conv2d Layer with fan_in:1, fan_out:6, kernel_size:5', 'A Max Pooling with kernel size:2', 'A Relu', 'A Conv2d Layer with fan_in:6, fan_out:12, kernel_size:5', 'A Max Pooling with kernel size:2', 'A Relu', 'A reshape layer', 'A Linear Layer With Size (192, 120)', 'A Relu', 'A Linear Layer With Size (120, 10)']\n",
      "                \n",
      "(16, 10)\n",
      "(16, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import mynn as nn\n",
    "import numpy as np\n",
    "\n",
    "model = nn.models.Model_CNN([(1, 6, 5), (2,), (6, 12, 5), (2,),(\"reshape\"),(12*4*4, 120),(120, 10)], \"ReLU\", [1e-4, 1e-4, 1e-4, 1e-4])\n",
    "linear_model = nn.models.Model_MLP([28*28, 600, 10], 'ReLU', [1e-4, 1e-4])\n",
    "print(model)\n",
    "X = np.random.randn(16, 1, 28, 28)\n",
    "Y = model(X)\n",
    "print(Y.shape)\n",
    "print(model.backward(Y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.29796387  0.04534184 -0.96975248  0.34969088 -1.68670888]\n",
      "   [-1.38366456  0.77444898 -1.24438968 -0.38563044 -0.46548582]\n",
      "   [ 1.4563753   0.63595711  1.18212638  1.13751325  0.20759483]\n",
      "   [-1.65763602 -0.45781169  0.18384728 -1.15566621  1.10085234]\n",
      "   [ 0.64629089 -0.66509908 -0.35892644  0.32152349 -0.77909116]]]]\n",
      "[[[[1.4563753]]]]\n",
      "[[[[0.        0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]\n",
      "   [1.4563753 0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]]]]\n"
     ]
    }
   ],
   "source": [
    "import mynn as nn\n",
    "import numpy as np\n",
    "maxpool = nn.op.MaxPool(3)\n",
    "X = np.random.randn(1, 1, 5, 5)\n",
    "Y = maxpool(X)\n",
    "print(X)\n",
    "print(Y)\n",
    "grad = maxpool.backward(Y)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8124003410339355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "A = np.random.randn(10240, 1024)\n",
    "B = np.random.randn(1024, 10240)\n",
    "s = time.time()\n",
    "A@B\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([[ 0.3642,  0.3642,  0.3642,  0.3642,  0.3642,  0.3642],\n",
      "        [-2.0319, -2.0319, -2.0319, -2.0319, -2.0319, -2.0319],\n",
      "        [ 0.5698,  0.5698,  0.5698,  0.5698,  0.5698,  0.5698],\n",
      "        [-1.1332, -1.1332, -1.1332, -1.1332, -1.1332, -1.1332]],\n",
      "       requires_grad=True)\n",
      "xt: tensor([[ 1.2254],\n",
      "        [-1.9671],\n",
      "        [ 2.0343],\n",
      "        [-2.1155]], grad_fn=<AddBackward0>)\n",
      "dxt_dt: tensor([[ 0.6214,  0.6214,  0.6214,  0.6214,  0.6214,  0.6214],\n",
      "        [-0.1773, -0.1773, -0.1773, -0.1773, -0.1773, -0.1773],\n",
      "        [ 0.6899,  0.6899,  0.6899,  0.6899,  0.6899,  0.6899],\n",
      "        [ 0.1223,  0.1223,  0.1223,  0.1223,  0.1223,  0.1223]])\n",
      "Expected dxt_dt: tensor([[ 3.7285,  3.7285,  3.7285,  3.7285,  3.7285,  3.7285],\n",
      "        [-1.0638, -1.0638, -1.0638, -1.0638, -1.0638, -1.0638],\n",
      "        [ 4.1397,  4.1397,  4.1397,  4.1397,  4.1397,  4.1397],\n",
      "        [ 0.7335,  0.7335,  0.7335,  0.7335,  0.7335,  0.7335]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 构造测试数据\n",
    "batchsize = 4\n",
    "t = torch.randn(batchsize, 1)\n",
    "t = t.repeat(1, 6) # 扩展为 (batchsize, 6)\n",
    "t.requires_grad = True  # 设置 t 为需要梯度的变量\n",
    "new_t = t.sum(dim=1, keepdim=True)/6\n",
    "xt = new_t**2 + 3*new_t  # 假设 xt = t^2 + 3t\n",
    "xt = xt.expand(4, 6)  # 扩展为 (batchsize, 6)\n",
    "xt = torch.zeros_like(xt, requires_grad=True)  # 假设 xt = t^2 + 3t\n",
    "xt = new_t**2 + 3*new_t  # 假设 xt = t^2 + 3t\n",
    "# t = t.expand(4, 6)  # 扩展为 (batchsize, 6)\n",
    "\n",
    "# 计算 dxt_dt\n",
    "dxt_dt = torch.autograd.grad(\n",
    "    outputs=xt,\n",
    "    inputs=t,\n",
    "    grad_outputs=torch.ones_like(xt),\n",
    "    # is_grads_batched=True,\n",
    "    retain_graph=True\n",
    ")[0]\n",
    "\n",
    "print(\"t:\", t)\n",
    "print(\"xt:\", xt)\n",
    "print(\"dxt_dt:\", dxt_dt)\n",
    "\n",
    "# 理论值：dxt_dt 应为 2*t + 3\n",
    "expected = 2 * t + 3\n",
    "# expected = expected.repeat(1, 6)\n",
    "print(\"Expected dxt_dt:\", expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HHL\n"
     ]
    }
   ],
   "source": [
    "class HHL():\n",
    "    def __init__(self):\n",
    "        self.a =1\n",
    "\n",
    "hhh = HHL()\n",
    "\n",
    "print(hhh.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
