{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4704513749915966\n",
      "[[ 0.08010519  0.09514217 -0.27446077  0.09705294  0.07291801  0.1262416\n",
      "   0.12853563  0.1381744   0.10687867  0.06588128]\n",
      " [ 0.14426688 -0.13856374  0.07343536  0.07732372  0.08723299  0.06565907\n",
      "   0.10860828  0.08513169  0.10661724  0.17211681]\n",
      " [ 0.11519213  0.05843734  0.05659112  0.07515481 -0.48571856  0.13614891\n",
      "   0.09966839  0.11968198  0.13456721  0.11096082]\n",
      " [ 0.11087483  0.104292    0.09989931 -0.43177009  0.12091445  0.05666207\n",
      "   0.12156603  0.13936521  0.09501132  0.06301911]\n",
      " [ 0.14572514  0.07599145  0.10150195  0.08129988  0.06561162  0.16228628\n",
      "  -0.1209137   0.12373146  0.07167487  0.09853964]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mynn import op as nn\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "# test 1 # \n",
    "# input = np.random.randn(batchsize, 10)\n",
    "# l1 = nn.Linear(10, 20)\n",
    "# l1(input).shape\n",
    "# grad = np.random.randn(batchsize, 20)\n",
    "# l1.backward(grad).shape\n",
    "\n",
    "# test 2 #\n",
    "def test2(verbose=False):\n",
    "    iter_time = 10000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(X)\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "        l2.backward(grad)\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 1*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"Need to break!!!\") \n",
    "        # print([np.linalg.norm(l2.params[key]) for key in l2.grads.keys()])\n",
    "    print(f\"the residual norm is {np.linalg.norm(l2.W-W_gt), np.linalg.norm(l2.b-b_gt)}\")\n",
    "\n",
    "def test3(verbose=False):\n",
    "    iter_time = 200000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l1 = nn.Linear(10, 10)\n",
    "    relu = nn.ReLU()\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(relu(l1(X)))\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "\n",
    "        # passing the grad to l2!\n",
    "        passing_grad = l2.backward(grad)\n",
    "        print(f\"norm of the grad is {np.linalg.norm(grad)}\")\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 0.01*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(f\"grad is {grad}\")\n",
    "                raise ValueError(\"l2 Need to break!!!\") \n",
    "            \n",
    "        # passing the grad to relu!\n",
    "        passing_grad = relu.backward(passing_grad)\n",
    "        # no params to optimize for relu!\n",
    "\n",
    "        # passing the grad to l1!\n",
    "        l1.backward(passing_grad)\n",
    "        print(np.linalg.norm(passing_grad))\n",
    "        for key in l1.grads.keys():\n",
    "            l1.params[key] -= 0.01*l1.grads[key]\n",
    "            if np.any(np.isnan(l1.params[key])):\n",
    "                print(l1.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"l1 Need to break!!!\")        \n",
    "\n",
    "def test4():\n",
    "    con1 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=4)\n",
    "    con2 = nn.Conv2D(in_channels=6, out_channels=12, kernel_size=5, stride=2)\n",
    "    X = np.random.rand(16, 3, 32, 32)\n",
    "    print(con2(con1(X)).shape)\n",
    "    grad = np.zeros((16, 12, 13, 13))\n",
    "    con2.backward(grad)\n",
    "\n",
    "def test5():\n",
    "    predict = np.random.rand(5, 10)\n",
    "    lable = np.array([2, 1, 4, 3, 6])\n",
    "    loss = nn.MultiCrossEntropyLoss()\n",
    "    print(loss(predicts=predict, labels=lable))\n",
    "    loss.backward()\n",
    "    print(loss.grads)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.58902126, 0.84982927, 0.69148555, 0.79715837, 0.86140474,\n",
       "        0.8477196 , 1.16029094, 1.37419002, 0.75400499, 0.89468823],\n",
       "       [1.52677724, 0.70761547, 0.79329229, 0.95238444, 1.56455597,\n",
       "        1.71611036, 0.66965285, 1.67495572, 1.56626208, 1.02444736],\n",
       "       [1.47392092, 0.57078069, 1.0675779 , 1.10062624, 1.34603732,\n",
       "        1.80266524, 0.72181436, 1.3427381 , 1.29061461, 0.7701871 ],\n",
       "       [0.82875284, 1.02353493, 1.14056256, 1.06572518, 1.71726292,\n",
       "        1.44589808, 1.58676943, 0.94848372, 0.7140915 , 0.86007213],\n",
       "       [0.73105419, 0.65518691, 0.59307837, 1.15775071, 1.47799324,\n",
       "        0.9675821 , 0.68992062, 0.72303462, 1.084457  , 0.54069814],\n",
       "       [0.71945183, 0.84364523, 1.00120862, 0.64481229, 0.78763621,\n",
       "        1.10130791, 1.61132444, 0.96329192, 0.66052967, 0.53621996],\n",
       "       [0.82698881, 1.19407872, 0.68600773, 1.05863821, 0.70824527,\n",
       "        0.92217503, 0.79053727, 1.61909287, 0.74721811, 0.67159098],\n",
       "       [1.48766046, 0.58227073, 0.61665516, 0.60336524, 0.86259075,\n",
       "        0.82242605, 1.14366392, 0.65776528, 1.4999322 , 0.91322674],\n",
       "       [0.66699918, 0.99668343, 0.89459856, 0.65495594, 0.93267262,\n",
       "        1.06289262, 0.81813808, 0.80253108, 1.48692774, 1.28009666],\n",
       "       [1.33700869, 1.08272457, 0.83017104, 0.59470432, 0.86305884,\n",
       "        1.0271721 , 1.37605518, 1.41222906, 1.37135489, 1.19901289]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(10, 10)\n",
    "np.exp(a)/np.exp(a).mean(axis=1,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
