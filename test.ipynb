{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 12, 10, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mynn import op as nn\n",
    "\n",
    "batchsize = 16\n",
    "\n",
    "# test 1 # \n",
    "# input = np.random.randn(batchsize, 10)\n",
    "# l1 = nn.Linear(10, 20)\n",
    "# l1(input).shape\n",
    "# grad = np.random.randn(batchsize, 20)\n",
    "# l1.backward(grad).shape\n",
    "\n",
    "# test 2 #\n",
    "def test2(verbose=False):\n",
    "    iter_time = 10000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(X)\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "        l2.backward(grad)\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 1*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"Need to break!!!\") \n",
    "        # print([np.linalg.norm(l2.params[key]) for key in l2.grads.keys()])\n",
    "    print(f\"the residual norm is {np.linalg.norm(l2.W-W_gt), np.linalg.norm(l2.b-b_gt)}\")\n",
    "\n",
    "def test3(verbose=False):\n",
    "    iter_time = 200000\n",
    "    N = 16*iter_time\n",
    "    X_data = np.random.randn(N, 10)\n",
    "    W_gt = np.random.randn(10, 20)\n",
    "    b_gt = np.random.randn(20)\n",
    "    Y_data = X_data@W_gt + b_gt\n",
    "    l1 = nn.Linear(10, 10)\n",
    "    relu = nn.ReLU()\n",
    "    l2 = nn.Linear(10, 20)\n",
    "    for i in range(iter_time):\n",
    "        X = X_data[i*16:(i+1)*16, :]\n",
    "        Y_gt = Y_data[i*16:(i+1)*16, :]\n",
    "        Y_pred = l2(relu(l1(X)))\n",
    "        loss = np.linalg.norm(Y_gt-Y_pred)\n",
    "        if verbose:\n",
    "            print(loss)\n",
    "        grad = -(Y_gt-Y_pred)\n",
    "\n",
    "        # passing the grad to l2!\n",
    "        passing_grad = l2.backward(grad)\n",
    "        print(f\"norm of the grad is {np.linalg.norm(grad)}\")\n",
    "        for key in l2.grads.keys():\n",
    "            l2.params[key] -= 0.01*l2.grads[key]\n",
    "            if np.any(np.isnan(l2.params[key])):\n",
    "                print(l2.grads[key])\n",
    "                print(f\"grad is {grad}\")\n",
    "                raise ValueError(\"l2 Need to break!!!\") \n",
    "            \n",
    "        # passing the grad to relu!\n",
    "        passing_grad = relu.backward(passing_grad)\n",
    "        # no params to optimize for relu!\n",
    "\n",
    "        # passing the grad to l1!\n",
    "        l1.backward(passing_grad)\n",
    "        print(np.linalg.norm(passing_grad))\n",
    "        for key in l1.grads.keys():\n",
    "            l1.params[key] -= 0.01*l1.grads[key]\n",
    "            if np.any(np.isnan(l1.params[key])):\n",
    "                print(l1.grads[key])\n",
    "                print(grad)\n",
    "                raise ValueError(\"l1 Need to break!!!\")        \n",
    "\n",
    "def test4():\n",
    "    con1 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=5)\n",
    "    con2 = nn.Conv2D(in_channels=6, out_channels=12, kernel_size=5, stride=2)\n",
    "    X = np.random.rand(16, 3, 28, 28)\n",
    "    print(con2(con1(X)).shape)\n",
    "    # grad = np.zeros((16, 12, 13, 13))\n",
    "    # con2.backward(grad)\n",
    "\n",
    "def test4_backward(verbose=False):\n",
    "    iter_time = 1000\n",
    "    con1 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=4)\n",
    "    con2 = nn.Conv2D(in_channels=3, out_channels=6, kernel_size=4)\n",
    "    print(np.linalg.norm(con1.kernel-con2.kernel))\n",
    "    print(np.linalg.norm(con1.b-con2.b))\n",
    "    for i in range(iter_time):\n",
    "        X = np.random.rand(16, 3, 32, 32)\n",
    "        gt = con1(X)\n",
    "        pred = con2(X)\n",
    "        grad = -(gt-pred)\n",
    "        con2.backward(grad)\n",
    "        for key in con2.grads.keys():\n",
    "            con2.params[key] -= 0.1*con2.grads[key]\n",
    "        if verbose and i%100==0:\n",
    "            print(np.linalg.norm(con1.b-con2.b))\n",
    "    print(np.linalg.norm(con1.kernel-con2.kernel))\n",
    "    print(np.linalg.norm(con1.b-con2.b))\n",
    "\n",
    "\n",
    "\n",
    "def test5():\n",
    "    predict = np.random.rand(5, 10)\n",
    "    lable = np.array([2, 1, 4, 3, 6])\n",
    "    loss = nn.MultiCrossEntropyLoss()\n",
    "    print(loss(predicts=predict, labels=lable))\n",
    "    loss.backward()\n",
    "    print(loss.grads)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # test4_backward(verbose=True)\n",
    "    # test2()\n",
    "    test4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CNN Model With Whose Sublayer is as below:\n",
      "                    ['A Conv2d Layer with fan_in:3, fan_out:12, kernel_size:5', 'A Relu', 'A Conv2d Layer with fan_in:12, fan_out:12, kernel_size:5', 'A Linear Layer With Size (120, 10)']\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "import mynn as nn\n",
    "\n",
    "model = nn.models.Model_CNN([(3, 12, 5), (12, 12, 5), (120, 10)], \"ReLU\", [1e-4, 1e-4, 1e-4])\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.29796387  0.04534184 -0.96975248  0.34969088 -1.68670888]\n",
      "   [-1.38366456  0.77444898 -1.24438968 -0.38563044 -0.46548582]\n",
      "   [ 1.4563753   0.63595711  1.18212638  1.13751325  0.20759483]\n",
      "   [-1.65763602 -0.45781169  0.18384728 -1.15566621  1.10085234]\n",
      "   [ 0.64629089 -0.66509908 -0.35892644  0.32152349 -0.77909116]]]]\n",
      "[[[[1.4563753]]]]\n",
      "[[[[0.        0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]\n",
      "   [1.4563753 0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]\n",
      "   [0.        0.        0.        0.        0.       ]]]]\n"
     ]
    }
   ],
   "source": [
    "import mynn as nn\n",
    "import numpy as np\n",
    "maxpool = nn.op.MaxPool(3)\n",
    "X = np.random.randn(1, 1, 5, 5)\n",
    "Y = maxpool(X)\n",
    "print(X)\n",
    "print(Y)\n",
    "grad = maxpool.backward(Y)\n",
    "print(grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
